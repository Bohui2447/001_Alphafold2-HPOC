# 001_Alphafold2-HPOC

## Important Links

The repo for ColabFold in your local PC is here: https://github.com/YoshitakaMo/localcolabfold

The tutorial for running ColabFold notebook on a HPC over SSH is here: https://www.jameslingford.com/blog/colabfold-hpc-ssh-howto/

The repo for SPOC is here: https://github.com/walterlab-HMS/SPOC

# Run ColabFold

### Create .sh file: 
``` nano protein1_protein2.sh ```

### Copy and paste the following script in:

# This code will submit the the same number of jobs to gpu as to the number of predictions you want to make. 
# You can find the colabfold ouput in the same folder as the fasta sequence file. 
```bash    
#!/bin/bash
#BSUB -q gpu
#BSUB -R "rusage[mem=20G]"
#BSUB -J "predict[1-208]"  # number of protein-pairs
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -n 1
#BSUB -W 2:00

# Base directory (from your screenshot)
BASE_DIR="/user/All_multier" # change the user foder based on your hpc account

# Auto-generate input list from folder structure
INPUT_LIST=($(ls -d $BASE_DIR/MLP3B_HUMAN_*_HUMAN))
FOLDER="${INPUT_LIST[$LSB_JOBINDEX-1]}"  # LSF arrays are 1-based

# Get FASTA file (you only have one per folder)
FASTA="$FOLDER/pair_$(basename $FOLDER).fasta"
BASENAME=$(basename $FASTA .fasta)

# Redirect logs to each folder
exec > "$FOLDER/${BASENAME}.out" 2> "$FOLDER/${BASENAME}.err"

# Load module and run
module load localcolabfold/1.5.5
LOCALCOLABIMG=/share/pkg/containers/localcolabfold/localcolabfold_1.5.5.sif

echo "Processing $FASTA in $FOLDER"
singularity exec --nv $LOCALCOLABIMG colabfold_batch \
    --templates \
    --num-recycle 3 \
    --num-ensemble 1 \
    --num-models 3 \
    "$FASTA" "$FOLDER"
```

### SPOC Set-up
## What is SPOC and why we use it?

### Clone the SPOC repository
- To clone the SPOC repository, use the following command:
  ``` bash
  git clone https://github.com/walterlab-HMS/SPOC.git
  ```
- Navigate into the cloned directory: ```cd SPOC```

### Create environment to load necessary dependencies

If you are using conda or miniconda, please refer to original repo.

- create the environment:
  ```bash
  conda env create -f SPOC/environment.yml
  ```
- activate the environment:
  ``` bash
  conda activate spoc_venv
  ```

### Run SPOC 
Here is an example input folder:
```bash
my_afm_predictions_folder/
│-- DONS_HUMAN__MCM3_HUMAN__1374aa.a3m.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_scores_rank_001_alphafold2_multimer_v3_model_1_seed_000.json.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_scores_rank_002_alphafold2_multimer_v3_model_2_seed_000.json.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_scores_rank_003_alphafold2_multimer_v3_model_3_seed_000.json.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_unrelaxed_rank_001_alphafold2_multimer_v3_model_1_seed_000.pdb.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_unrelaxed_rank_002_alphafold2_multimer_v3_model_2_seed_000.pdb.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_unrelaxed_rank_003_alphafold2_multimer_v3_model_3_seed_000.pdb.xz

```
- contains a file with .a3m (which is same for all the model)
- three .json files and three .pdb files generated by colabfold
- Run the prediction by
```bash
python3 run.py my_afm_predictions_folder
```

---
### Run Batch files for colabfold and SPOC

### Example Script for a low number of tasks at once (<20):
```bash
    #!/bin/bash
    # Go to the folder that contains all the folders for fasta file
    cd All_multimer
    
    # Loop through each folder containing the FASTA file
    for dir in */; do
        # Remove trailing slash to get folder name
        folder=${dir%/}
    
        # Find the fasta file inside the folder
        fasta=$(find "$folder" -maxdepth 1 -name "*.fasta" | head -n 1)
    
        # Skip if no fasta file found
        if [[ -z "$fasta" ]]; then
            echo "No FASTA in $folder, skipping..."
            continue
        fi
    
        # Set job and output file base names based on folder
        base_name=$(basename "$fasta" .fasta)
    
        # Generate a temporary job script
        job_script="${folder}/run_colabfold.bsub"
    
        cat > "$job_script" <<EOF
    #!/bin/bash
    #BSUB -q gpu
    #BSUB -R "rusage[mem=20G]"
    #BSUB -J ${base_name}_models
    #BSUB -gpu "num=1"
    #BSUB -n 1
    #BSUB -W 2:00
    #BSUB -oo ${folder}/${base_name}.out
    #BSUB -eo ${folder}/${base_name}.err
    
    module load localcolabfold/1.5.5
    LOCALCOLABIMG=/share/pkg/containers/localcolabfold/localcolabfold-1.5.5.sif
    
    singularity exec --nv $LOCALCOLABIMG colabfold_batch \
         --templates --num-recycle 3 --num-ensemble 1 --num-models 3 "$fasta" "${folder}"
    
    EOF
    
        # Submit the job
        bsub < "$job_script"
    done

```
### Example Script for a large number of tasks, e.g., ~2000
- Step 1: Make sure you have the run_wrapper.py in the same directory as the run.py

  Create a new sh file within the SPOC folder: ```nano run_wrapper.py```

  Copy and paste the following script into the file:

  This code will create a temporary directory to only take the a3m, .json, and .pdb files from the colabfold output and runs run.py on thie temporary directory
    ```bash
    # run_wrapper.py
    import os
    import sys
    import glob
    import subprocess
    import tempfile
    import re
    def run_prediction(input_folder):
        input_folder = os.path.abspath(input_folder)
        folder_name = os.path.basename(input_folder)
        # Match only top-level files
        a3m_files = glob.glob(os.path.join(input_folder, '*.a3m'))
        json_files = sorted(glob.glob(os.path.join(input_folder, '*_scores_rank_*_model_*_seed_000.json')))
        pdb_files = sorted(glob.glob(os.path.join(input_folder, '*_unrelaxed_rank_*_model_*_seed_000.pdb')))
        all_files = a3m_files + json_files + pdb_files
    
        if not all_files:
            print(f"[{input_folder}] No valid input files found.")
            return
    
        # Temporary clean directory with only relevant files
        with tempfile.TemporaryDirectory() as tmpdir:
            for file_path in all_files:
                link_path = os.path.join(tmpdir, os.path.basename(file_path))
                os.symlink(os.path.abspath(file_path), link_path)
    
            print(f"[{input_folder}] Running prediction...")
            output_file = f"{folder_name}_SPOC_output.csv"
            subprocess.run(["python3", "run.py", tmpdir,"--output",output_file], cwd=os.path.dirname(__file__), check=True)
    
    if __name__ == "__main__":
        if len(sys.argv) < 2:
            print("Usage: python run_wrapper.py <folder1> [<folder2> ...]")
            sys.exit(1)
    
        for folder in sys.argv[1:]:
            run_prediction(folder)
    ```
- Step 2: In the same directory of your All_Multimer folder, get all the folder names of the fasta files into the input_folders.txt
    ```bash
    find All_multimer -maxdepth 1 -type d -not -path 'All_multimer' > input_folders.txt
    ```

- Step 3: Create a file at the same directory as the input_folders.txt:```nano run_prediction.sh```

    Copy and paste into the file (be sure to change the directory in the script!):
    ```bash
#!/bin/bash
#BSUB -q large
#BSUB -n 40
#BSUB -J spoc_predict
#BSUB -W 10:00
#BSUB -oo spoc_predict.out
#BSUB -eo spoc_predict.err
#BSUB -R "rusage[mem=8000]"

# Setup conda
source ~/.bashrc
conda activate spoc_venv


RUN_WRAPPER_PATH="/home/bohui.li3-umw/Bohui/004_sx_apex_alfold3/SPOC/run_wrapper.py"
cd /home/bohui.li3-umw/Bohui/004_sx_apex_alfold3/All_multimer

for folder in */; do
    folder=${folder%/}
    echo "Running on $folder"
    python3 "$RUN_WRAPPER_PATH" "$folder"
```
    Now you have a list of SPOC predictions within the same directory as the run.py (which is your SPOC folder)
